<!DOCTYPE html>
<html>
<head>
  <title>Machine Learning for Scientists</title>
  <meta charset="utf-8">
  <meta name="description" content="Machine Learning for Scientists">
  <meta name="author" content="Jonathan Dursi">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="assets/img/oicr-trans.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Machine Learning for Scientists</h1>
    <h2>An Introduction</h2>
    <p>Jonathan Dursi<br/>Scientific Associate/Software Engineer, Informatics and Biocomputing, OICR</p>
  </hgroup>
    <a href="https://github.com/ljdursi/ML-for-scientists/zipball/gh-pages" class="example">
     Download
    </a>
  <article></article>  
  <footer class = 'license'>
    <a href='http://creativecommons.org/licenses/by-nc-sa/3.0/'>
    <img width = '80px' src = 'http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png'>
    </a>
  </footer>
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <article data-timings="">
    <style type="text/css">
.title-slide {
  background-color: #EDEDED; 
}
article p {
  text-align:left;
}
p {
  text-align:left;
}
img {     
  max-width: 90%
}
</style>

<script type="text/javascript" src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>

<script type="text/javascript">
$(function() {     
  $("p:has(img)").addClass('centered'); 
  });
</script>
 

<h2>Purpose of This Course</h2>

<p>You should leave here today:</p>

<ul>
<li>Having some basic familiarity with key terms,</li>
<li>Having used a few standard fundamental methods, and have a grounding in the underlying theory,</li>
<li>Having developed some familiarity with the python package <code>scikit-learn</code></li>
<li>Understanding some basic concepts with broad applicability.</li>
</ul>

<p>We&#39;ll cover, and you&#39;ll use, most or all of the following methods:</p>

<table><thead>
<tr>
<th align="right"></th>
<th align="left">Supervised</th>
<th align="left">Unsupervised</th>
</tr>
</thead><tbody>
<tr>
<td align="right">CONTINUOUS</td>
<td align="left"><strong>Regression</strong>:  OLS, Lowess, Lasso</td>
<td align="left"><strong>Variable Selection</strong>: PCA</td>
</tr>
<tr>
<td align="right">DISCRETE</td>
<td align="left"><strong>Classification</strong>:  Logistic Regression, kNN, Decision Trees, Naive Bayes, Random Forest</td>
<td align="left"><strong>Clustering</strong>: k-Means, Hierarchical Clustering</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Techniques, Concepts</h2>
  </hgroup>
  <article data-timings="">
    <p class='..'>but more importantly, we&#39;ll look in some depth at these concepts:</p>

<div style='float:left;width:48%;' class='centered'>
  <ul>
<li>Bias-Variance</li>
<li>Resampling methods

<ul>
<li>Bootstrapping</li>
<li>Cross-Validation</li>
<li>Permutation tests</li>
</ul></li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <ul>
<li>Model Selection</li>
<li>Variable Selection</li>
<li>Multiple Hypothesis Testing</li>
<li>Geometric Methods</li>
<li>Tree-based Methods</li>
<li>Probabilistic methods</li>
</ul>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>ML and Scientific Data Analysis</h2>
  </hgroup>
  <article data-timings="">
    <p>Machine Learning is in some ways very similar to day-to-day scientific data analysis:</p>

<ul>
<li>Machine learning is model fitting.</li>
<li>First, data has to be:

<ul>
<li>put into appropriate format for tools,</li>
<li>quickly summarized/visualized as sanity check (&quot;data exploration&quot;),</li>
<li>cleaned</li>
</ul></li>
<li>Then some model is fit and parameters extracted.</li>
<li>Conclusions are drawn.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>ML vs Scientific Data Analysis</h2>
  </hgroup>
  <article data-timings="">
    <p>Many scientific problems being analyzed are already very well characterized.</p>

<ul>
<li>Model already defined, typically very solidily;</li>
<li>Estimating a small number of parameters within that framework.</li>
</ul>

<p>Machine learning is model fitting...</p>

<ul>
<li>But the model may be implicit,</li>
<li class='..'>and disposable.</li>
<li>The model exists to explore the data, or make improved predictions.</li>
<li>Generally not holding out for some foundational insight into the fundamental nature of our world.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Scientific Data Analysis with ML</h2>
  </hgroup>
  <article data-timings="">
    <p>But ML approaches can be very useful for science, particularly at the beginning of a research programme:</p>

<ul>
<li>Exploring data;</li>
<li>Uncovering patterns;</li>
<li>Testing models.</li>
</ul>

<p>Having said that, there are some potential gotchas:</p>

<ul>
<li>Different approaches, techniques than common in scientific data analysis.  Takes some people-learning.</li>
<li>When not just parameters but the model itself up for grabs, one has to take care not to lead oneself astray.

<ul>
<li>Getting &quot;a good fit&quot; is not in question when you have all possible models devised by human minds at your disposal.  But does it mean anything?</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Types of problems</h2>
  </hgroup>
  <article data-timings="">
    <p>Broadly, data analysis problems fall into two categories:</p>

<ul>
<li><strong>Supervised</strong>: already have some data labelled with (approximately) the right answer.

<ul>
<li><em>e.g.</em>, curve fitting</li>
<li>For prediction.  &quot;Train&quot; the model on known data, predict on new unlabelled data.</li>
</ul></li>
<li><strong>Unsupervised</strong>: discover patterns in data.

<ul>
<li>What groups of items in this data are similar?  Different?</li>
<li>For exploration, evaluation, and prediction if useful.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Types of Data</h2>
  </hgroup>
  <article data-timings="">
    <p>And we will be working on two broad classes of variables:</p>

<ul>
<li><strong>Continuous</strong>: real numbers.</li>
<li><strong>Discrete</strong>: 

<ul>
<li>Binary: True/False, On/Off.</li>
<li>Categorical: Item falls into category A, category B...</li>
<li>Ordinal: Discrete, but has an intrinsic order.  (Low, Med, High; S, M, L, XL).</li>
</ul></li>
</ul>

<p>Others are possible too -- intervals, temporal or spatial continuous data -- but we won&#39;t be considering those today.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="segue dark" id="slide-8" style="background:;">
  <hgroup>
    <h2>Regression</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>We&#39;re going to spend this morning discussing regression.</p>

<ul>
<li>It&#39;s the most familiar to most of us; so</li>
<li>It&#39;s a good place to introduce some concepts we&#39;ll find useful through the rest of the day.</li>
</ul>

<p>In regression problems, </p>

<ul>
<li>Data comes in as a set of \(n\) observations.</li>
<li>Each of which has \(p\) &quot;features&quot;; we&#39;ll be considering all-continuous input features, which isn&#39;t necessarily the case.</li>
<li>And the initial data we have also has a known &quot;endogenous&quot; value, the \(y\)-value we&#39;re trying to fit.</li>
<li>Goal is to learn a function \(y = \hat{f}(x_1, x_2, \dots, x_p)\) for predicting new values.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Ordinary Least Squares (OLS)</h2>
  </hgroup>
  <article data-timings="">
    <p>As we learned on our grandmothers knee, a good way to fit a functional
form \(\hat{y} = \hat{f}(\vec{x};\theta)\) to some data \((\vec{x},y)\)
is to minimize the squared error.  We assume the data is generated
by some true function</p>

<p>\[
y = f(\vec{x}) + \epsilon
\]</p>

<p>where \(\epsilon\) is some irreducible error (here assumed constant), and we choose \(\theta\):</p>

<p>\[
\hat{\theta} = \mathrm{argmin}_\theta \sum_i \left ( y_i - \hat{f} (x_i, \theta) \right )^2.
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Note on Loss Functions</h2>
  </hgroup>
  <article data-timings="">
    <p>Here we&#39;re going to be using least squares error as the function
to minimize.  But this is not the only <em>loss</em> <em>function</em> one could
usefully optimize.</p>

<p>In particular, least-squares has a number of very nice mathematical
properties, but it puts a lot of weight on outliers.  If the residual
\(r_i = y_i - \hat{y}_i\) and the loss function is \(l = \sum_i
\rho(r_i)\), some &quot;robust regression&quot; methods use different methods:</p>

<p>\[ 
\begin{eqnarray*} 
\rho_\mathrm{LS}(r_i) & = &  r_i^2 \\
\rho_\mathrm{Huber}(r_i) & = & \left \{ \begin{array}{ll} r_i^2 & \mathrm{if } |r_i| \le c; \\ c(2 r_i - c) & \mathrm{if } |r_i| > c. \\ \end{array} \right . \\
\rho_\mathrm{Tukey}(r_i) & = & \left \{ \begin{array}{ll} r_i \left ( 1 - \frac{r_i}{c} \right )^2 & \mathrm{if } |r_i| \le c; \\ 0 & \mathrm{if } |r_i| > c. \\ \end{array} \right . \\
\end{eqnarray*} 
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Note on Loss Functions</h2>
  </hgroup>
  <article data-timings="">
    <p>Even crazier loss functions are possible --- for your particular
application, it may be worse to over-predict than under-predict
(for instance), so the loss function needn&#39;t necessarily be symmetric
around \(r_i = 0\).</p>

<p>The &quot;right&quot; loss function is problem-dependent.</p>

<p>For now, we&#39;ll just use least-squares, as it&#39;s familar and the
mechanics don&#39;t change with other loss functions.  However, different
algorithms may need to be used to use different loss functions;
many explicitly use the nice mathematical properties of least
squares.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Polynomial Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>Let&#39;s do some linear regression of a noisy, tilted sinusoid:</p>

<pre><code class="python">import scripts.regression.biasvariance as bv
import numpy
import matplotlib.pylab as plt

x,y = bv.noisyData(npts=40)
p = numpy.polyfit(x, y, 1)
fitfun = numpy.poly1d(p)

plt.plot(x,y,&#39;ro&#39;)
plt.plot(x,fitfun(x),&#39;g-&#39;)
plt.show()
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Polynomial Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>We should have something that looks like the figure on the right.</p>

<p>Questions we should be asking ourselves whenever we&#39;re doing something like this:</p>

<ul>
<li>How is this likely to perform on new data?</li>
<li>How is the accuracy likely to be in the centre (\(x = 0\))?  At \(x = 2\)?</li>
<li>How robust is our prediction - how variable is it likely to be if we had gotten different data?</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/bias-variance/linear-fit.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Polynomial Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Repeat the same steps with degree 20. </p>

<p>We can get much better accuracy at our points if we use a higher-order polynomial.</p>

<p>Ask ourselves the same questions:</p>

<ul>
<li>How is this likely to perform on new data?</li>
<li>How is the accuracy likely to be in the centre (\(x = 0\))?  At \(x = 2\)?</li>
<li>How robust is our prediction - how variable is it likely to be if we had gotten different data?</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/bias-variance/twentyth-fit.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Polynomial Regression - In Sample Error</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>We can generate our fit and then calculate the error on the data we&#39;ve used to train:</p>

<p>\[ 
E = \sum_i \left ( y_i - \hat{f}(x_i) \right )^2
\]</p>

<p>and if we plot the results, we&#39;ll see the error monotonically going down with the degree of polynomial we use.  So should we use high-order polynomials?</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/bias-variance/in-sample-error-vs-degree.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Bias-Variance Decomposition</h2>
  </hgroup>
  <article data-timings="">
    <p>Consider the squared error we get from fitting some regression model \(\hat{f}(x)\) to some given set of data \(x\):</p>

<p>\[
\begin{eqnarray*}
E \left [ (y-\hat{y})^2 \right ] & = & E \left [ ( f + \epsilon - \hat{f})^2 \right ] \\
              & = & E \left [ \left ( f - \hat{f} \right )^2 \right ] + \sigma_\epsilon^2 \\
              & = & E \left [ \left ( (f - E[\hat{f}]) - (\hat{f} - E[\hat{f]}) \right )^2 \right ] + \sigma_\epsilon^2 \\
              & = & E \left [ \left ( f - E[\hat{f}] \right )^2 \right ]  - 2 E \left [ ( f - E[\hat{f}]) (\hat{f} - E[\hat{f}]) \right ] + E \left [ \left (\hat{f} - E[\hat{f}]) \right )^2 \right ] + \sigma_\epsilon^2 \\
              & = & \left (E[f] - E[\hat{f}]) \right )^2 + E \left [ \left ( \hat{f} - E[\hat{f}] \right )^2 \right ]  + \sigma_\epsilon^2 \\
              & = & \mathrm{Bias}^2 + \mathrm{Var} + \sigma_\epsilon^2
\end{eqnarray*}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Bias-Variance Decomposition</h2>
  </hgroup>
  <article data-timings="">
    <p>\[
\mathrm{MSE} = \left (E[f] - E[\hat{f}]) \right )^2 + E \left [ \left ( \hat{f} - E[\hat{f}] \right )^2 \right ]  + \sigma_\epsilon^2  = \mathrm{Bias}^2 + \mathrm{Var} + \sigma_\epsilon^2
\]</p>

<ul>
<li>Last term: intrinisic noise in the problem.  Can&#39;t do anything about it; we won&#39;t consider it any further right now.</li>
<li>First term: <strong>bias</strong> squared of our estimate.

<ul>
<li>Is the expectation value of our regression estimate \(\hat{f}\), the expectation value of \(f\)?</li>
</ul></li>
<li><p>Second term: <strong>variance</strong> of our estimate.</p>

<ul>
<li>How robust/variable is our regression estimate, \(\hat{f}\)?</li>
</ul></li>
<li><p>Obvious: mean squared error has contribution from both of these terms.</p></li>
<li><p>Less obvious: there is almost always a tradeoff between bias and variance.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Bias, Variance in Polynomial Fitting</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Because this is synthetic data, we can examine bias and variance in our regression estimates:</p>

<ul>
<li>Generate many sets of data from the model</li>
<li>For each one,

<ul>
<li>Generate fit with given degree</li>
<li>Plot fit for visualization</li>
<li>Generate predictions at a given point (say, \(x=0\))</li>
</ul></li>
<li>Examine bias, variance of predictions around zero.</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/bias-variance/lin-bias-variance.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Polynomial Fitting - Constant</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/bias-variance/const-bias-variance.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Polynomial Fitting - Linear</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/bias-variance/lin-bias-variance.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Polynomial Fitting - Seventh Order</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/bias-variance/seventh-bias-variance.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Polynomial Fitting - Tenth Order</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/bias-variance/tenth-bias-variance.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Polynomial Fitting - Twentyth Order</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/bias-variance/twentyth-bias-variance.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Bias and Consistency</h2>
  </hgroup>
  <article data-timings="">
    <p>Bias is a measure of how consistent the model is with the true behaviour.</p>

<p>Very generally, models that are too simple can&#39;t capture all of the
behaviour of the system, and so estimators based on these simple models
have higher bias.</p>

<p>As the complexity of model increases, bias tends to go down; the model
can capture whatever behaviour is seen.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Variance and Generalization</h2>
  </hgroup>
  <article data-timings="">
    <p>Variance is a measure of how sensitive the estimated model is to the particular
set of data it sees.</p>

<p>It is very closely tied to the idea of <strong>generalization</strong>.  Is the model learning
trends that will generalize to a new set of data?  Or is it just overfitting the 
noise of this particular data set?</p>

<p>As the complexity of a model increases, it tends to have higher variance; simple
models typically have very low variance.</p>

<p>Note too that variance typically gets smaller as sample sizes increase; a model
which shows large variance with different 100-point data sets will likely be
much better behaved on 1,000,000-point data sets.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Bias-Variance Tradeoff</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>For our polynomial example, if we compare the error in the computed
model with the &quot;true&quot; model (not generally available to us!), we can
plot the error vs the degree of the polynomial:</p>

<ul>
<li><p>For small degrees, the dominant term is bias; simpler models can&#39;t capture the true behaviour of the system.</p></li>
<li><p>For larger degrees, the dominant term is variance; more complex models are generalizing poorly and overfitting noise.</p></li>
</ul>

<p>There&#39;s some sweet spot where the two effects are comparably low.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/bias-variance/error-vs-degree.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Choosing the Right Model</h2>
  </hgroup>
  <article data-timings="">
    <p>(Or in this case, the &quot;metaparameters&quot; of the model)</p>

<p>In general, we get some dataset - we can&#39;t generate more data on a
whim, and we certainly can&#39;t just compare to the true model.</p>

<p>So what do we do to choose our model?  If we simply fit with higher
order polynomials and calculate the error on the same data set,
we&#39;ll find that more complex models are always &quot;better&quot;.</p>

<p>How do we do out-of-sample testing when we only have one set of samples?</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Training vs Validation</h2>
  </hgroup>
  <article data-timings="">
    <p>The solution is to hold out some of the data.  The bulk of the data is used for training 
the model; the remainder is used for validating the trained model against &quot;new&quot; data. </p>

<p>Once the model is chosen, then you train the selected model on the entire training+validiation
data set.</p>

<p>In some cases, you may need still further data; eg, you may need to both choose your
model, and end a paper or report with a sentence like &quot;the final model achieved 80% accuracy...&quot;.
This still can&#39;t be done on the data the model was trained on (train+validation); in that case,
a second chunk of data must be held out, for testing. </p>

<p>In the case of Training:Validation:Testing, a common breakdown of the data sizes might be 
50%:25%:25% of the initial set.  If you don&#39;t need a test data set, 2/3:1/3 is common.</p>

<p>Note that the data sets should be chosen randomly!</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Training and Validation Hold Out Data</h2>
  </hgroup>
  <article data-timings="">
    <p>Once a model has &quot;touched&quot; a hold-out data set, it&#39;s done.</p>

<p>If you iterate on models or model selection having touched all the
data, you&#39;re doing exactly the same thing as fitting a 50th-order
polynomial to 52 data points and saying &quot;See? Hardly any error!&quot;</p>

<ul>
<li>Once a model (family of models) has seen the validation data set, it&#39;s done; can&#39;t tune it any more.

<ul>
<li>Otherwise, risk eternal torment, gnashing of teeth, etc.</li>
</ul></li>
<li>Once a set of models (set of family of models) have been compared to on the test data set, they&#39;re done; no more model selection.

<ul>
<li>Eternal torment, gnashing of teeth, etc.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Hands-On: Model Selection</h2>
  </hgroup>
  <article data-timings="">
    <p>Use this validation-hold-out approach to generate a noisy data set, and choose a degree polynomial to fit the 
entire data set.  The routines <code>numpy.random.shuffle()</code> and <code>numpy.array_split()</code> may be helpful.</p>

<pre><code class="python">import scripts.regression.biasvariance as bv
x,y = bv.noisyData(npts=100)

import numpy
import numpy.random

a = numpy.arange(10)
numpy.random.shuffle(a)
print a
print numpy.array_split(a, 2)
</code></pre>

<pre><code>## [6 0 4 5 8 9 1 2 7 3]
## [array([6, 0, 4, 5, 8]), array([9, 1, 2, 7, 3])]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>\(k\)-fold Cross Validation</h2>
  </hgroup>
  <article data-timings="">
    <p>There are some downsides to the approach we&#39;ve taken for validation hold-out.  What if most negative outliers happened to be in the training set?</p>

<p>Ideally, would do several partitions, average over results.</p>

<p>\(k\)-fold Cross Validation:</p>

<ul>
<li>Partition data set (randomly) into \(k\) sets.</li>
<li>For each set:

<ul>
<li>Train on the remaining \(k-1\) sets</li>
<li>Validate on the held-out set</li>
</ul></li>
<li>Average results</li>
</ul>

<p>Makes very efficient use of the data set, easily automated.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>\(k\)-fold Cross Validation</h2>
  </hgroup>
  <article data-timings="">
    <p>How to choose \(k\)?</p>

<ul>
<li>\(k\) too large - the different training sets are very highly correlated (almost all of their points are the same).</li>
<li>\(k\) too small - don&#39;t get very much advantage of averaging.</li>
</ul>

<p>In practice, 10 is a very commonly-used value for \(k\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>\(k\)-fold Cross Validation</h2>
  </hgroup>
  <article data-timings="">
    <p>Let&#39;s look at <code>scripts/regression/crossvalidation.py</code>:</p>

<pre><code class="python">    err = 0.
    for i in range(kfolds):
        startv = n*i/kfolds; endv = n*(i+1)/kfolds
        test[idxes[startv:endv]] = True

        test_x  = x[test];  test_y  = y[test]
        train_x = x[-test]; train_y = y[-test]

        p = numpy.polyfit(train_x, train_y, d)
        fitf = numpy.poly1d(p)

        err = err + sum((test_y - fitf(test_x))**2)
        test[:] = False
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>\(k\)-fold Cross Validation</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Running gives:</p>

<pre><code class="python">from scripts.regression import crossvalidation 

crossvalidation.chooseDegree(50)
</code></pre>

<p>This chooses the degree to fit 50 data points using
cross validation, and tests the results on a new 50 points.</p>

<p>The error is estimated for each degree, and the minimum is chosen.
In practice, may not want to greedily go for the minimum; the
simplest model that is &quot;close enough&quot; to the minimum is generally
a good choice.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/crossvalidation/CV-polynomial.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Resampling Aside #1</h2>
  </hgroup>
  <article data-timings="">
    <h3>Bootstrapping</h3>

<p>Cross-validation is closely related to a more fundamental method, bootstrapping. </p>

<p>Let&#39;s say you want to find how some function of your data would behave - say, the range of sample means of your data, or
a mean and standard deviation of an estimation error for a given model (as with CV).</p>

<p>You&#39;d like new sets of data that you could calculate your statistic on, and then look at the distribution of those.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>Parametric Bootstrapping</h2>
  </hgroup>
  <article data-timings="">
    <p>If you <em>know</em> the form of the distribution that describes your data, you can simulate new data sets:</p>

<ul>
<li>Fit the distribution to the data;</li>
<li>Generate synthetic data sets from the now-known distribution to your heart&#39;s content;</li>
<li>Calculate the statistic on these synthetic data sets, and get their distribution.</li>
</ul>

<p>This works perfectly well if you know a model that will correctly describe your data; and indeed if you do know that, 
it would be madness <em>not</em> to make use of it in your analysis.</p>

<p>But what if you don&#39;t?</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>Non-parametric Bootstrapping</h2>
  </hgroup>
  <article data-timings="">
    <p>The key insight to the non-parametric bootstrap is that you already have an unbiased description of the process that generated your data - the data itself.</p>

<p>The apprach for the non-parametric bootstrap is:</p>

<ul>
<li>Generate synthetic data sets from the original by resampling;</li>
<li>Calculate the statistic on these synthetic data sets, and get their distribution.</li>
</ul>

<p>This is less powerful than parametric bootstrapping <strong>if</strong> the parametric functional form is correctly known; but it is much better if the parametric
functional form is incorrectly known, or not known at all.</p>

<p>Cross-validation is a particular case: CV takes \(k\) (sub)samples of the original data set, applied a function (fit the data set to part, calculate error on the remainder), 
and calcluates the mean.  </p>

<p>Bootstrapping can be used far more generally.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>Non-parametric Bootstrapping</h2>
  </hgroup>
  <article data-timings="">
    <p>Example:</p>

<pre><code class="python">import numpy
import numpy.random
numpy.random.seed(789)
data = numpy.random.randn(100)*2 + 1  # Normal with sigma=2, mu=1

print numpy.mean(data)
means = [ numpy.mean( numpy.random.choice(data,100) ) for i in xrange(1000) ]
print numpy.mean(means)
print numpy.var(means)   # expect: sigma^2/N = 4/100 = 0.04
</code></pre>

<pre><code>## 1.03332758651
## 1.02269633225
## 0.0395873095222
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Hands On: Boostrapping Forest Fires</h2>
  </hgroup>
  <article data-timings="">
    <p>In the file <code>scripts/boostrap/forestfire.py</code> is a routine which will load historical data
about forest fires in northeastern Portugal, including the area burned.  You can view it
as follows:</p>

<pre><code class="python">import matplotlib.pylab as plt
import scripts.bootstrap.forestfire as ff

t, h, w, r, area = ff.forestFireData(skipZeros=True)

n, bins, patches = plt.hist( area, 50, facecolor=&#39;red&#39;, normed=True, log=True )
plt.xlabel(&#39;Area burned (Hectares)&#39;)
plt.ylabel(&#39;Frequency&#39;)
plt.show()
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Hands On: Boostrapping Forest Fires</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/bootstrap/area-histogram.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h2>Hands On: Boostrapping Forest Fires</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Using the non-parametric bootstrap, calculate the 5% and 95%
confidence intervals for the median of the area burned in forest
fires large enough to have their areas recorded.  Eg, you would
expect that in 90% of similar data sets, the median would fall
within those confidence intervals.</p>

<p>You should get a distribution of medians that look like the plot 
on the right:</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/bootstrap/median-area-histogram.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Regression as Smoothing</h2>
  </hgroup>
  <article data-timings="">
    <p>Regression is just a form of data smoothing - smoothing the data onto a particular functional form.</p>

<p>For instance, consider OLS linear regression on variables we&#39;ve `centred&#39; - subtracted off the mean.</p>

<p>The OLS regression function is
\[
\hat{y}(x) = \frac{\sum_i x_i y_i}{\sum_j x_j^2} x
\]
We can rewrite this as
\[
\hat{y}(x) = \sum_i y_i \left ( \frac{ x_i x}{\sum_j x_j^2} \right ) = \sum_i y_i w(x,\vec{x})
\]</p>

<p>That is, a weighted average of <em>all</em> of the initial $y_i$s.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Regression with Nonparametric Smoothing</h2>
  </hgroup>
  <article data-timings="">
    <p>To get good prediction results, we don&#39;t <em>need</em> to choose a particular, parameterized, global functional form to smooth onto.</p>

<p>As with parametric/nonparametric bootstrap,</p>

<ul>
<li>Parametric version is more powerful <em>if</em> the functional form is correctly known.</li>
<li>Nonparametric version has to &quot;waste&quot; some information to reconstruct the overall shape.</li>
</ul>

<p>Many nonparametric approaches are quite local, allowing adaptation to local features.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Nonparametric Regression - Kernel</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <ul>
<li>The simplest approach is to smooth the value of each point over some local area with some kernel basis function.</li>
<li>The regression function is then calculated by summing the input data points convolved with the basis function.</li>
<li>Commonly used kernel basis functions:

<ul>
<li>Gaussian (downside - never falls off to zero)</li>
<li>Tricubic </li>
</ul></li>
<li>Kernel basis function characterized by some bandwidth.</li>
<li>How to choose best bandwidth?</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/nonparametric/kernel-demo.png" alt="">
<img src="outputs/nonparametric/kernel-fit.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Nonparametric Regression - LOWESS</h2>
  </hgroup>
  <article data-timings="">
    <p>LOESS and LOWESS are two very related methods which use a similar-but-different approach to kernel regression.</p>

<div style='float:left;width:48%;' class='centered'>
  <p>At each point, a <strong>local</strong> low-order polynomial regression performed, fit to some fixed <strong>number</strong> of neighbouring points, rather than smoothed over a particular area.</p>

<p>Number of neighbours has to be chosen:</p>

<ul>
<li>Too few: too noisy.</li>
<li>Too many: smooth too much.</li>
</ul>

<p>How are number of neighbours chosen?</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/nonparametric/lowess-fit.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2><code>statsmodels</code></h2>
  </hgroup>
  <article data-timings="">
    <p>Statsmodels ( <a href="http://statsmodels.sourceforge.net">http://statsmodels.sourceforge.net</a> ) is a very useful python package which includes:</p>

<ul>
<li>Wide variety of regression tools</li>
<li>Ties in with pandas and other packages, to give very nice environment for exploring these sorts of questions</li>
<li>Many tools for hypothesis testing</li>
</ul>

<pre><code class="python">def lowessFit(x,y,**kwargs):
    &quot;&quot;&quot;Use statsmodels to do a lowess fit to the given data.
       Returns the x and fitted y.&quot;&quot;&quot;
    fit = sm.nonparametric.lowess(y,x,**kwargs)
    xlowess = fit[:,0]
    ylowess = fit[:,1]
    return xlowess, ylowess
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Nonparametric Regression</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>You will sometimes hear nonparametric techniques referred to as &quot;model-free&quot;.

<ul>
<li>These people are either 

<ul>
<li>dishonest, or </li>
<li>too dumb to understand the techniques they&#39;re using.</li>
</ul></li>
<li>Either way, not to be trusted.</li>
</ul></li>
<li>Have very specific models, and thus applicability:

<ul>
<li>Tend to require constant noise (&quot;homoskedastic&quot;)</li>
<li>Kernel methods work best when data has roughly constant density</li>
</ul></li>
<li>However, the underlying models are both:

<ul>
<li>normally weaker than specifying a particular functional form</li>
<li>fairly transparent - if these methods go wrong, they don&#39;t go subtly wrong.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Parametric vs Nonparametric Regression</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>If you know the right answer, you should certainly exploit that

<ul>
<li>Will get more information out of your data.</li>
</ul></li>
<li>Nonparametric techniques will always have higher variance, bias than the right parametric method, if available

<ul>
<li>But it&#39;s hard for them to go subtly badly wrong.</li>
</ul></li>
<li>Even if you think you know the right answer and do a parametric regression, you should do a non-parametric regession, as well.

<ul>
<li>Costs almost nothing.</li>
<li>If your parametric regression is correct, non-parametric version should look like a noisier version of same.</li>
<li>If they differ substantially in some region, now you&#39;ve learned something potentially quite interesting.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Final Notes on Regression ... For Now</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Always consider nonparametric regression alongside any parametric regression you run.</li>
<li>It should go without saying (grandmother&#39;s knee again) that, whatever regression you run, you should investigate the residuals:

<ul>
<li>Unbiased?</li>
<li>Randomly (normally) distributed?</li>
<li>Constant amplitude?</li>
</ul></li>
<li>Otherwise, eternal torment, gnashing of teeth, etc.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="segue dark nobackground" id="slide-51" style="background:;">
  <hgroup>
    <h2>Classification</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Classification</h2>
  </hgroup>
  <article data-timings="">
    <p>Classification is a broad set of problems that supperficially look a lot like regression:</p>

<ul>
<li>Get some data in, with known correct answers</li>
<li>Learn algorithm that produces new correct answers for new inputs.</li>
</ul>

<p>But it is greatly complicated by the fact that the labels are discrete:</p>

<ul>
<li>Item should either be in category A or category B.</li>
<li>You don&#39;t get partial points for being close; there&#39;s no category A\(\frac{1}{2}\).</li>
</ul>

<p>So classification algorithms spend a great deal of time looking for methods to as cleanly
distinguish various categories as possible.</p>

<p>In some cases, it may be useful to have not only a &quot;best guess&quot; classification, but a quantitative
measure of how much evidence there is in that assignment.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Classification Problems</h2>
  </hgroup>
  <article data-timings="">
    <p>Some classic classification problems:</p>

<ul>
<li>Bioinformatics - classify proteins according to their function</li>
<li>Image processing - what objects exist in the image</li>
<li>Text categorization:

<ul>
<li>Spam filtering</li>
<li>Sentiment analysis: is this tweet about my company positive or negative?</li>
</ul></li>
<li>Fraud detection</li>
<li>Market segmentation</li>
</ul>

<p>Input variables are commonly both continuous and categorical.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Binary vs Multiclass Classification</h2>
  </hgroup>
  <article data-timings="">
    <p>Classification algorithms can be broken broadly into two categories;</p>

<ul>
<li>Binary classification: answers yes/no questions about whether an item is in a particular class;</li>
<li>Multiclass classification: of \(m\) classes, which one does this item belong to?</li>
</ul>

<p>One approach to multiclass classification is to decompose into \(m\) binary classification problems, and
for each item, choose the category in which the item is most securely assigned.  (This turning a discrete variable 
into a series of binary variables is called &quot;binarization&quot;, and arises in other contexts).  </p>

<p>But inherently multiclass methods also exist.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Outline</h2>
  </hgroup>
  <article data-timings="">
    <p>For the next couple of hours, we&#39;ll look at the following:</p>

<ul>
<li>Decision Trees</li>
<li>Evaluating binary classifiers

<ul>
<li>Confusion matrix</li>
<li>Precision, Recall</li>
<li>ROC curves</li>
</ul></li>
<li>Nearest Neighbours</li>
<li>Logistic Regression</li>
<li>Naive Bayes</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Decision Trees</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>A Decision Tree is a structure, and the algorithm which generates
it, which classifies an input based on a number of binary decisions.</p>

<p>&quot;Splits&quot; the data set based on one of the \(p\) features of the items.</p>

<p>Can split based on continuous data (&quot;if height &lt; 1.5 m&quot;), or ordinal/discrete
(&quot;if category == &#39;A&#39;).</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/basic.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Batman Decision Tree</h2>
  </hgroup>
  <article data-timings="">
    <p>Consider this example ( from Rob Schapire, Princeton:)</p>

<table><thead>
<tr>
<th>Name</th>
<th>cape</th>
<th>ears</th>
<th>male</th>
<th>mask</th>
<th>smokes</th>
<th>tie</th>
<th>Good/Bad</th>
</tr>
</thead><tbody>
<tr>
<td>Batman</td>
<td>True</td>
<td>True</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>Good</td>
</tr>
<tr>
<td>Robin</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>Good</td>
</tr>
<tr>
<td>Alfred</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>Good</td>
</tr>
<tr>
<td>Pengin</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>True</td>
<td>Bad</td>
</tr>
<tr>
<td>Catwoman</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>Bad</td>
</tr>
<tr>
<td>Joker</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>Bad</td>
</tr>
</tbody></table>

<p>&quot;Learn&quot; a decision tree to classify new characters into Good/Bad.</p>

<p>How does your tree do on this test set?</p>

<table><thead>
<tr>
<th>Name</th>
<th>cape</th>
<th>ears</th>
<th>male</th>
<th>mask</th>
<th>smokes</th>
<th>tie</th>
<th>Good/Bad</th>
</tr>
</thead><tbody>
<tr>
<td>Batgirl</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>??</td>
</tr>
<tr>
<td>Riddler</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>??</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Splitting Algorithms</h2>
  </hgroup>
  <article data-timings="">
    <p>Consider the following two possible first splits:</p>

<ul>
<li>Split based on cape.

<ul>
<li>Cape == True: get Batman, Robin (Good)</li>
<li>Cape == False: get Alfred (Good), Penguin, Catwoman, Joker (Bad)</li>
</ul></li>
<li>or Split based on tie.

<ul>
<li>Tie == True: get Alfred (Good), Penguin (Bad)</li>
<li>Tie == False: get Batman, Robin (Good), Catwoman, Joker (Bad).</li>
</ul></li>
</ul>

<p>There&#39;s a sense in which cape is clearly the better split.  It leads to two groups, one of which is purely good, and the
other which is almost purely bad.</p>

<p>The other choice gives you two groups just as heterogeneous as the input data.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Splitting Algorithms</h2>
  </hgroup>
  <article data-timings="">
    <p>Typically rank possible splits based on increase in &quot;purity&quot; of the two subgroups it generates.</p>

<p>Information theory: this is clearly a more informitive decision, leads two two groups of much lower entropy.</p>

<div style='float:left;width:48%;' class='centered'>
  <p>Consider the probability \(p\) that a member of one of the subgroups is in a given category.  Two common measures 
for the &quot;impurity&quot; of the groups generated (higher is worse):</p>

<ul>
<li>Gini Index: \(p (1 - p)\)</li>
<li>Entropy: \(-p \ln p - (1-p) \ln (1 - p)\)</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/impurity-plots.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Splitting Algorithms</h2>
  </hgroup>
  <article data-timings="">
    <p>So splitting algorithms look something like this:</p>

<ul>
<li>Until every element is in a pure subtree,

<ul>
<li>For each variable, consider a split.

<ul>
<li>For categorical, consider all levels; split and measure impurity of resulting groups.</li>
<li>For continuous, use line optimization to choose best point at which to split, keep track of impurity at that point.</li>
</ul></li>
<li>Choose split which maximizes (negative) change in impurity</li>
</ul></li>
</ul>

<p>Actually implementing this leads to a large number of decisions
which effect both quality of results and computational performance.
Common, classic decision tree algorithms you will encounter include
CART and C4.5.  Both (and many others) are fine, but have their
particular strengths and weaknesses.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Scikit-learn</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Scikit-learn is a python package which contains a suite of machine
learning algorithms each of which are implemented with a consistent API.</p>

<p>Makes trying various algorithms on a data set fairly painless.</p>

<p>Typical supervised-learning workflow:</p>

<ul>
<li>Instantiate an instance of a particular learner (here, a classifier), with whatever parameters you choose;</li>
<li>Use the <code>fit()</code> method to train it for some given input training data;</li>
<li>use the <code>predict()</code> method to apply it to some test data.</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/good-evil.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2><code>sklearn</code></h2>
  </hgroup>
  <article data-timings="">
    <p>Let&#39;s take a look at <code>scripts/classification/decisiontree.py</code>:</p>

<pre><code class="python">import pandas
import sklearn
import sklearn.tree

#...

    train, labels, test = goodEvilData()
    model = sklearn.tree.DecisionTreeClassifier()
    model.fit(train, labels)

    predictions = model.predict(test)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2><code>pandas</code></h2>
  </hgroup>
  <article data-timings="">
    <p>Pandas is a python module for handling data series and R-like data frames.</p>

<p>Data frames are collections of columns of data about observations.
Each column may be of a different type (string, integer, float,
boolean), but within a column the data is homogeneous.</p>

<p>This isn&#39;t typically super-useful for regression (where we often, but not always, are
using continuous values), but is generally pretty useful for classification problems.</p>

<p>Many other useful features, but data frames are what interest us today.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2><code>pandas</code></h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">import pandas

df = pandas.DataFrame( {&#39;Name&#39;:[&#39;Jonathan&#39;,&#39;Ted&#39;,&#39;Harvey&#39;],
                        &#39;Type&#39;:[&#39;human&#39;,&#39;dog&#39;,&#39;roomba&#39;],
                        &#39;Height&#39;:[180.,50.,5.],
                        &#39;Age&#39;:[42, 15, 2],
                        &#39;Recharged&#39;:[False,True,True]} )
print df
</code></pre>

<pre><code>##    Age  Height      Name Recharged    Type
## 0   42     180  Jonathan     False   human
## 1   15      50       Ted      True     dog
## 2    2       5    Harvey      True  roomba
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2>Trees and Overfitting</h2>
  </hgroup>
  <article data-timings="">
    <p>As with polynomials and regression, can easily produce overly-complex models which do great on their training data set
but don&#39;t generalize.</p>

<p>Except this is guaranteed to happen with trees.  Tree will <strong>always</strong> give you a completely divided up data set that will
correctly classify the input data set.   </p>

<p>How to avoid this?</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2>Tree-pruning</h2>
  </hgroup>
  <article data-timings="">
    <p>The normal approach is to let the tree do its thing, and then
afterwards prune it at some level where the results are &quot;good enough&quot;
while the model is not &quot;too complex&quot;.</p>

<p>How to determine where that point is?  </p>

<p>Cross validation.</p>

<p>Note that after pruning, have a way to express confidence in classification - \(p\) in the chosen leaf.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2>Hands on - Iris Data Set</h2>
  </hgroup>
  <article data-timings="">
    <p>The iris data set is famous enough that it has its own wikipedia page: <a href="http://en.wikipedia.org/wiki/Iris_flower_data_set">http://en.wikipedia.org/wiki/Iris_flower_data_set</a> .</p>

<p>It is a set of four measurements for each of 50 irises of 3 different species.  (At least 100 of which were picked in Quebec).
It&#39;s a classic classification problem - two of the categories seperate themselves quite cleanly, but the third is much trickier.</p>

<p>Play with the tree parameters and see if you can improve on the sklearn defaults.</p>

<pre><code class="python">import scripts.classification.decisiontree as dt

a = dt.irisProblem()
b = dt.irisProblem(splitter=&#39;random&#39;)
</code></pre>

<pre><code>## Misclassifications on test set:  3 out of  51
## Misclassifications on test set:  4 out of  51
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2>Nearest Neighbours - $k$NN</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Another approach to classification is very geometric in nature.</p>

<p>Given a new input observation, find the nearest point in the training set, and choose that classification.</p>

<p>A generalization is to choose the \(k\) nearest neighbours, and choose the classification that the majority of those \(k\) neighbouring
points has.</p>

<p>Case on the right: two normal distributions centred at \((-1,-1)\) and \((1,1)\) with \(\sigma = 3/2\).  </p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/knn-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>Bias-Variance in $k$NN</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/classification/knn-vary-k.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>Bias-Variance in $k$NN</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>There&#39;s a bias-variance-like tradeoff in $k$NN, that can be seen by varying \(k\) on the same data set on our right.</p>

<p>In words, what&#39;s happening as we increase \(k\)?</p>

<p>With \(k\) = 1, <strong>variance</strong> is very large.  The model is exceptionally sensitive to every single data point.
(See next slide).</p>

<p>But with large \(k\), we average over very large area - lose local features.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/knn-vary-k.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>Bias-Variance in $k$NN</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Here we see the variance of the model when \(k = 1\); we keep \(k\) fixed at one, but have different realizations of
the data set.  </p>

<p>The decision boundary varies widely from run to run.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/knn-variance.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>$k$NN and Geometry</h2>
  </hgroup>
  <article data-timings="">
    <p>Some notes on $k$NN:</p>

<ul>
<li>This method <strong>requires</strong> a distance metric.  This all but rules
out categorical input variables (<em>some</em> ordinal variables can be
made to work, but not all)</li>
<li>This can work extremely well in low-dimensional spaces (small
\(p\)).  In 1,000-dimensional space, for instance, for any meaningful
\(k\), many of your &quot;nearest&quot; neighbours may in fact be quite dissimilar.</li>
<li>Even for a modest number of continuous variables, some caution is needed: have to <em>scale</em> the variables.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>$k$NN and Geometry</h2>
  </hgroup>
  <article data-timings="">
    <p>Consider the variables in the iris data set</p>

<pre><code class="python">import sklearn.datasets
import numpy as np

iris = sklearn.datasets.load_iris()
for i in range(4):
    print iris.feature_names[i], &quot;variance: &quot;, np.round(np.var(iris.data[:,i]),2)
</code></pre>

<pre><code>## sepal length (cm) variance:  0.68
## sepal width (cm) variance:  0.19
## petal length (cm) variance:  3.09
## petal width (cm) variance:  0.58
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>Scaling continuous features</h2>
  </hgroup>
  <article data-timings="">
    <p>Petal length varies over a <em>much</em> greater range than sepal width.</p>

<p>If we just use euclidian distance, sepal width will provide very
little information - essentially all points are close in that
dimension.</p>

<p>Want to scale variables so they all have the opportunity to play equal role. 
A common technique is to centre the variables by subtracting off their means, then
scale by the standard deviation:</p>

<p>\[ 
X' = \frac{1}{\sigma_X} \left ( X - \mu \right )
\]</p>

<p>Many libraries will do this for you for methods where it matters, but not all; 
check the documentation!</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>Digits data set</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>The digits data set is a set of ~1,800 handwritten digits, ~180 of
each digit, used to train OCR systems (originally, for US zip codes).</p>

<p>In <code>sklearn.datasets.load_digits()</code>.</p>

<p>Each image is represented by 64 greylevel values (8x8 grid).</p>

<p>A simple $k$NN test on this data set is found in <code>scripts/classification/knndigits.py</code>.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/digits.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>Hands-on: kNN digits</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">import scripts.classification.knndigits as knnd

knnd.digitsProblem(weights=&#39;distance&#39;)
</code></pre>

<pre><code>## Number mismatched:  12 / 611
## [[62  0  0  0  0  0  0  0  0  0]
##  [ 0 58  0  0  0  0  0  0  0  0]
##  [ 0  0 62  1  0  0  0  0  0  0]
##  [ 0  0  0 59  0  0  0  0  0  0]
##  [ 0  0  0  0 68  0  0  0  0  0]
##  [ 0  0  0  0  0 52  0  0  0  1]
##  [ 0  0  0  0  0  0 62  0  0  0]
##  [ 0  0  0  0  0  0  0 59  0  0]
##  [ 0  4  0  1  0  0  0  0 57  0]
##  [ 0  0  0  3  0  1  0  0  1 60]]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>Hands-on: kNN digits</h2>
  </hgroup>
  <article data-timings="">
    <p>Play with the arguments to KNeighboursClassifier ( particularly
useful optins: n_neighbors, weights, algorithm ).  Can you improve
the performance?</p>

<p>(You may wish to pass a seed to keep the train/test set the same for each test)</p>

<p>You may also wish to try to use a decision tree on the same data.  Is that 
better?  Worse?  Why?</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>Confusion matrix</h2>
  </hgroup>
  <article data-timings="">
    <p>How you &#39;score&#39; a classifier is different than a regression.</p>

<p>You can count the number wrongly classified, and that is a useful
way to keep score - but it doesn&#39;t give you much information you can use to improve the result.</p>

<p>Confusion matrix tells you which misclassifications happened.
Traditionally, &quot;true&quot; classifications on the rows, model predictions
on the columns.</p>

<pre><code class="python">import scripts.classification.decisiontree as dt

dt.irisProblem(printConfusion=True)
</code></pre>

<pre><code>## Misclassifications on test set:  2 out of  51
## [[19  0  0]
##  [ 0 17  0]
##  [ 0  2 13]]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>Evaluating Binary Classifiers</h2>
  </hgroup>
  <article data-timings="">
    <p>Binary classification is a common and important enough special case
that its confusion matrix elements have special names, and various quality measures are defined.</p>

<table><thead>
<tr>
<th></th>
<th>Classified Positive (CP)</th>
<th>Classified Negative (CN)</th>
</tr>
</thead><tbody>
<tr>
<td>Actual Positive (P)</td>
<td><span style="color:green; font-weight:bold">True Positive(TP)</span></td>
<td><span style="color:red; font-weight:bold">False Negative (FN)</span></td>
</tr>
<tr>
<td>Actual Negative (N)</td>
<td><span style="color:red; font-weight:bold">False Positive (FP)</span></td>
<td><span style="color:green; font-weight:bold">True Negative (TN)</span></td>
</tr>
</tbody></table>

<p>One can always get exactly one of FN or FP zero - for instance, if
my classifier just classifies everything as positive, there will
never be a false negative, and vice versa.</p>

<p>But there&#39;s usually some tradeoff between false negatives and false
positives.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>Evaluating Binary Classifiers</h2>
  </hgroup>
  <article data-timings="">
    <p>Several quality measures exist, but because of the FN/FP tradeoff, you normally need two, one from each category:</p>

<ul>
<li>Something to do with how many of the actual positives you get

<ul>
<li>Sensitivity = Recall = True Postive Rate (TPR)

<ul>
<li>\(\mathrm{TPR} = \mathrm{TP}/\mathrm{P} = \mathrm{TP}/\left(\mathrm{TP}+\mathrm{FN}\right)\).<br></li>
<li>Given an actual positive, what is the probability of it being classified positive?</li>
</ul></li>
</ul></li>
<li>Something to do with how strong evidence your positive classification is:

<ul>
<li>Specificity (SPC) = 1 - False Positive Rate (FPR)

<ul>
<li>\(\mathrm{SPC} = \mathrm{TN}/\mathrm{N} = \mathrm{TN}/\left (\mathrm{FP}+\mathrm{TN}\right)\)</li>
<li>Given an actual negative, what is the probability of it being classified negative?</li>
</ul></li>
<li>Precision = Positive Predictive Value (PPV): \(\mathrm{PPV} = \mathrm{TP}/\left(\mathrm{TP}+\mathrm{FP}\right)\).

<ul>
<li>What fraction of your positive calls are true?</li>
</ul></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>ROC Curve</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>In most binary classifiers, there&#39;s some equivalent of a threshold you can set; </p>

<ul>
<li>Set it lower (to allow more true positives, but also false positives);</li>
<li>Or higher (to allow more true negatives, but also false negatives).</li>
</ul>

<p>Plotting one of the two quality measures on either axis, get a ROC curve.  </p>

<ul>
<li>Diagonal line = random chance</li>
<li>Want to be as far away as possible.</li>
</ul>

<p>sklearn plots TPR vs FPR (= 1 - SPC).</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/roc.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>ROC Curve</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="outputs/classification/roc.png" alt=""></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>Evaluating Binary Classifiers</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Where to set the threshold?</p>

<ul>
<li>For applications where a false positive is just as bad as a false negative, try to balance the two error rates.</li>
<li>But some applications, a false positive is much worse than a false negative, or vice versa.</li>
<li>Correct choice is problem dependant.</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/roc.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-84" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>One way to consider binary classification is to go back to regression,
and consider a linear regression to an integer 0/1 variable for classification.</p>

<p>Get over 0.5, True, else False.</p>

<p>Requires a linear seperation between the classes, but this is
somewhat less of a problem for high-\(p\) problems; can often be useful.</p>

<p>However, naive linear regresion has a number of problems, which grow worse
at high \(p\), that mostly come down to the unbounded nature of the function.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/logistic-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-85" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>However, a surprisingly minor variation on this approach makes it very useful.</p>

<p>A whole infrastructure exists for &quot;generalized linear models&quot;, where the function
being fit is not 
\[
y = \beta_0 + x_1 \beta_1 + x_2 \beta_2 + \dots = x \vec{\beta}
\]</p>

<p>but some power or exponential of \(X \vec{\beta}\).</p>

<p>Consider fitting not \(p\), but the log-odds,
\[
\mu = \ln \frac{p}{1-p} = \beta_0 + \beta_1 x_1
\]
so now \(p\) can remain between 0 and 1, and the linear function can remain unbounded.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/logistic-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-86" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>We can fit this log-odds equation, and derive
\[
p = \frac{e^{\beta_0 + x \beta_1}}{1 + e^{\beta_0 + x \beta_1}} = \frac{1}{1 - e^{-(\beta_0 + x \beta_1)}}
\]</p>

<p>This approach has a number of very nice properties:</p>

<ul>
<li>We have a nice, bounded, well-behaved function and transformation to fit</li>
<li>We can directly calculate the inferred probability of membership.<br></li>
<li>We&#39;re essentially fitting a Bernoulli process.</li>
</ul>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/logistic-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-87" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>One has to use somewhat different numerical algorithms to fit these
curves; typical curve-fitting algorithms deal very poorly with
exponentials.</p>

<p>Often use techinques like expectation maximization (EM) or other
well-conditioned iterative methods.  </p>

<p>That&#39;s fine; they&#39;re all hidden beneath whatever logistic or GLM 
packages you might want to use.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/logistic-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>Logistic Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Logistic regression can be fairly easily used in multiclassification problems; we have the probabilities in the binary case, so
we can apply \(c\) classification tests and take the best one.</p>

<p>Note the linear class boundaries.  The results aren&#39;t as bad as they look here, as we&#39;re projecting out two dimensions.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/classification/logistic-iris-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>Logistic Regression Hands on</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">import scripts.classification.logisticiris as logir
import numpy.random

numpy.random.seed(123)
logir.irisProblem(printConfusion=True)
</code></pre>

<pre><code>## Misclassifications on test set:  6 out of  51
## [[12  0  0]
##  [ 0 16  6]
##  [ 0  0 17]]
</code></pre>

<p>Because of the additional dimensions in the problem, logistic
regression actually does quite well on this problem.  Can you tweak
it to work still better?  Useful parameters to play with are the
penalty parameter (using L1 or L2 distances), a regularization parameter
C, a tolerance (when to stop iterating).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>Naive Bayes</h2>
  </hgroup>
  <article data-timings="">
    <p>Naive Bayes might be better called &quot;Naive Application of Conditional Probability&quot;, but you can see where
that might not have caught on.</p>

<p>A classifier that calculates an assignment probability given data \(\vec{x}\) calculates, implicitly or explicitly, a probability
\[
P(C=c | \vec{x}) = P(C=c | x_1, x_2, \cdots, x_p )
\]</p>

<p>By Bayes&#39;s Theorem (conditional probability), this is</p>

<p>\[
\mathrm{P}(C=c | x_1, x_2, \cdots, x_p ) = \frac{\mathrm{P}(C=c) \mathrm{P}(x_1, x_2, \cdots, x_p | C=c)}{\mathrm{P}(x_1, x_2, \cdots, x_n)}
\]</p>

<p>The bottom is just a normalization we&#39;re not interested in, so we consider
\[
\mathrm{P}(C=c | x_1, x_2, \cdots, x_p ) \propto \mathrm{P}(C=c) \mathrm{P}(x_1, x_2, \cdots, x_p | C=c)
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>Naive Indeed</h2>
  </hgroup>
  <article data-timings="">
    <p>\[
\mathrm{P}(C=c | x_1, x_2, \cdots, x_p ) \propto \mathrm{P}(C=c) \mathrm{P}(x_1, x_2, \cdots, x_p | C=c)
\]</p>

<p>Here&#39;s where the &quot;Naive&quot; comes in.  We&#39;re going to assume that the different features of the data are <em>independent</em>
of each other, conditional on \(C=c\).  Madness!  But, recklessly tossing caution to the wind,</p>

<p>\[
\mathrm{P}(C=c | x_1, x_2, \cdots, x_p ) \propto \mathrm{P}(C=c) \prod_{i=1}^p \mathrm{P}(x_i | C=c)
\]</p>

<p>By making the decision to completely ignore the correlations between features, this method is blissfully unaware
of the primary difficulty of high-dimensional (high-\(p\)) datasets, and training Naive Bayes classifiers becomes extremely easy.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-92" style="background:;">
  <hgroup>
    <h2>Training Naive Bayes</h2>
  </hgroup>
  <article data-timings="">
    <p>Looking back at the Batman data set:</p>

<table><thead>
<tr>
<th>Name</th>
<th>cape</th>
<th>ears</th>
<th>male</th>
<th>mask</th>
<th>smokes</th>
<th>tie</th>
<th>Good/Bad</th>
</tr>
</thead><tbody>
<tr>
<td>Batman</td>
<td>True</td>
<td>True</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>Good</td>
</tr>
<tr>
<td>Robin</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>Good</td>
</tr>
<tr>
<td>Alfred</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>Good</td>
</tr>
<tr>
<td>Pengin</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>True</td>
<td>Bad</td>
</tr>
<tr>
<td>Catwoman</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>Bad</td>
</tr>
<tr>
<td>Joker</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>Bad</td>
</tr>
</tbody></table>

<ul>
<li>\(\mathrm{P}(C=\mathrm{Good}) = 1/2\); \(\mathrm{P}(C=\mathrm{Bad}) = 1/2\); </li>
<li>\(\mathrm{P}(\mathrm{cape} | C=\mathrm{Good}) = 2/3\)</li>
<li>\(\mathrm{P}(\mathrm{ears} | C=\mathrm{Good}) = 1/3\)</li>
<li>\(\mathrm{P}(\mathrm{smokes} | C=\mathrm{Good}) = 0\)</li>
</ul>

<p>etc.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-93" style="background:;">
  <hgroup>
    <h2>Training Naive Bayes</h2>
  </hgroup>
  <article data-timings="">
    <p>In fact, what one would actually do is, rather than assume constants, is fit a distribution to the various conditional probabilities.</p>

<ul>
<li>Continuous: Gaussian (typically)</li>
<li>True/False: Bernoulli</li>
<li>Integer: Binomial/Multinomial</li>
</ul>

<p>But the basic training method is the same, and crucially, is done on each dimension independently.</p>

<p>Fast, requires quite modest amounts of data even in very high dimensional spaces.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-94" style="background:;">
  <hgroup>
    <h2>Naive Like a Fox</h2>
  </hgroup>
  <article data-timings="">
    <p>In fact, Naive Bayes can do quite well in problems where there are so many variables that the correlations between random
pairs of them are quite small.</p>

<p>A classic example is text processing, in which sometimes documents are treated as &quot;bags of words&quot;:</p>

<ul>
<li>&quot;twas brillig and the slithy toves did gyre and gimble in the wabe&quot;.</li>
<li>{ and:2, brillig:1, did:1, gimble:1, gyre:1, in:1, slithy:1, the:2, tove:1, twas:1, wabe:1 }</li>
</ul>

<p>Obviously, this throws away a great deal of semantically important information, but works quite well for broad-brush applications
like sentiment analysis, topic analysis, or spam filtering.</p>

<p>Individual word choices <em>are</em> most definitely correlated, but not incredibly strongly.  There are a lot of possible words out there.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-95" style="background:;">
  <hgroup>
    <h2>Usenet Newsgroups Classification</h2>
  </hgroup>
  <article data-timings="">
    <p>Usenet newsgroups were basically prehistoric Reddit, back in the
day.</p>

<p>Posts occurred in various topic forums, and the &quot;20 newsgroups
dataset&quot; ( <a href="http://qwone.com/%7Ejason/20Newsgroups/">http://qwone.com/~jason/20Newsgroups/</a> ) is a set of
~20,000 posts across these groups.  The question is one of
classification: could you correctly place a post in the right
newsgroup?</p>

<p>Very similar to any of a number of other text classification problems.</p>

<p>Scikit-Learn has a routine for loading this data set, and breaking
it into bags of words (and further processing, such as stripping
out relatively meaning-free English &quot;stop words&quot; that would otherwise 
flood the statistics: the, an, one, and, but, etc.)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-96" style="background:;">
  <hgroup>
    <h2>Usenet Newsgroups Classification</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">import scripts.classification.text as txt

categories = [&#39;comp.os.ms-windows.misc&#39;, &#39;misc.forsale&#39;, &#39;rec.motorcycles&#39;, &#39;talk.religion.misc&#39;]
txt.newsgroupsProblem(categories, printConfusion=True)
</code></pre>

<pre><code>## f1-score:  0.877293802813
## [[323  23  31  17]
##  [ 12 347  25   6]
##  [  3  13 370  12]
##  [  5   4  25 217]]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-97" style="background:;">
  <hgroup>
    <h2>Text Processing Hands On</h2>
  </hgroup>
  <article data-timings="">
    <p>Using <code>scripts/classification/text.py</code> as a startingpoint, play further with Naive Bayes (does BernoulliNB - just treating
the words as present/not present instead of counts - make a difference?).  Do the other classifiers work better?</p>

<p>Or: how does Naive Bayes deal with the Iris data set (using GaussianNB?)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-98" style="background:;">
  <hgroup>
    <h2>Classification Summary</h2>
  </hgroup>
  <article data-timings="">
    <p>We&#39;ve covered quite a bit of ground here:</p>

<ul>
<li>Probabilistic methods (Naive, maybe Logistic Regression)</li>
<li>Regression method (LR)</li>
<li>Tree-based method (decision tree)</li>
<li>Geometric method (kNN)</li>
</ul>

<p>They each have benefits and drawbacks.  Decision Trees and LR both,
in their way, require one or a series of linear separation surfaces;
kNN requires a meaningful distance metric.  Decision Trees and
especially NB can work quite well in high-dimensional spaces.</p>

<p>There are guidelines you can use, but ultimately experience and
experimentation is most important.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="dark segue nobackground" id="slide-99" style="background:;">
  <hgroup>
    <h2>Variable Selection</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-100" style="background:;">
  <hgroup>
    <h2>Multivariate Linear Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>Let&#39;s take a look at linear regression again, using statsmodels and a wider data set:</p>

<pre><code class="python">import statsmodels.api as sm

data = sm.datasets.star98.load()
X = data.exog; Y = data.endog
X = sm.add_constant(X)
model=sm.OLS(Y[:,0],X)
results = model.fit()
print results.summary()
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-101" style="background:;">
  <hgroup>
    <h2>Multivariate Linear Regression</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">##Model:                            OLS   Adj. R-squared:                  0.507
##[...]
##Time:                        13:01:10   Log-Likelihood:                -2279.5
##No. Observations:                 303   AIC:                             4601.
##Df Residuals:                     282   BIC:                             4679.
##[...]
##==============================================================================
##                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
##------------------------------------------------------------------------------
##const      -1.014e+04   5209.878     -1.946      0.053     -2.04e+04   115.090
##x1             0.2139      2.230      0.096      0.924        -4.175     4.603
##x2            15.3423      3.505      4.377      0.000         8.443    22.242
##x3             5.9944      4.140      1.448      0.149        -2.155    14.144
##x4            -3.0512      2.138     -1.427      0.155        -7.261     1.158
##x5          1597.6665    155.557     10.271      0.000      1291.467  1903.866
##x6          1176.8157    249.155      4.723      0.000       686.377  1667.255
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-102" style="background:;">
  <hgroup>
    <h2>Multivariate Linear Regression</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">##x7           340.5269     62.086      5.485      0.000       218.317   462.737
##x8         -1714.4834    904.058     -1.896      0.059     -3494.042    65.075
##x9          -405.7498    195.183     -2.079      0.039      -789.951   -21.549
##x10         -228.3569     99.588     -2.293      0.023      -424.388   -32.326
##x11            7.1600      4.862      1.473      0.142        -2.411    16.731
##x12            0.4313      1.333      0.324      0.747        -2.193     3.056
##x13         -109.8054     10.718    -10.245      0.000      -130.903   -88.708
##x14          -28.1694      2.587    -10.888      0.000       -33.262   -23.077
##x15          -21.4271      4.318     -4.963      0.000       -29.926   -12.928
##x16           84.6002     43.967      1.924      0.055        -1.945   171.146
##x17           47.8686     20.668      2.316      0.021         7.185    88.552
##x18           11.9591      4.679      2.556      0.011         2.750    21.168
##x19            1.9302      0.175     11.049      0.000         1.586     2.274
##x20           -2.5449      1.018     -2.499      0.013        -4.549    -0.540
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-103" style="background:;">
  <hgroup>
    <h2>Multivariate Linear Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>What to make of all of this?</p>

<ul>
<li>Can compute a linear regression with all available variables.</li>
<li>Is this meaningful?</li>
<li>Naively, it looks like some of these variables aren&#39;t very important.</li>
<li>Can construct a better-motivated, more robust model if we reduce the number of variables / features.</li>
<li>Some regression/clustering/classification algorithms will have real difficulties on problems with many extraneous variables.</li>
<li>Sometimes variables are highly collinear and will break some algorithms.</li>
<li>The information &quot;only these 5 (say) variables are really important&quot; is itself worth having.</li>
</ul>

<p>We should strive to produce the simplest model that produces sufficiently good answers.</p>

<ul>
<li>Feature selection.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-104" style="background:;">
  <hgroup>
    <h2>How Not to do Variable Selection</h2>
  </hgroup>
  <article data-timings="">
    <p>&quot;Variables 3,5,6,13,14,15, and 19 all have really low \(p\) values - let&#39;s just use those.&quot;</p>

<ul>
<li>No.</li>
</ul>

<p>A super-low \(p\)-value means, &quot;in <em>this</em> <em>model</em>, it&#39;s unlikely that this coefficient is zero&quot;.</p>

<ul>
<li>Says nothing about its contribution to the hypothetical model in which you&#39;ve removed other variables.</li>
<li>Says even less about how <em>important</em> the contribution of the variable is.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-105" style="background:;">
  <hgroup>
    <h2>How to do a Little Variable Selection</h2>
  </hgroup>
  <article data-timings="">
    <p>There are some filtering steps one can judiciously take to weed out some clearly irrelevant variables:</p>

<ul>
<li>Low variance filtering (<code>sklearn.feature_selection.VarianceThreshold</code>).
If the variable is essentially constant, it can&#39;t matter much to any model.</li>
<li>Univariate tests (<code>sklearn.feature_selection.SelectKBest</code>): for
supervised learning, <strong>if</strong> your data is sampled densely enough,
can do univariate tests on each parameter and remove ones that are sufficiently uncorrelated with label.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-106" style="background:;">
  <hgroup>
    <h2>How (Maybe, but Hopefully Not) to do Variable Selection</h2>
  </hgroup>
  <article data-timings="">
    <p>How have we done model selection in the past?</p>

<ul>
<li>Cross-validation!</li>
</ul>

<p>Huge CV problem:</p>

<ul>
<li>Try all \(2^p\) combinations of variables.</li>
<li>Regress on them.</li>
<li>Of the possibilities, find best (adjusted) error.</li>
<li>Will work for modest \(p\).</li>
<li>(But for modest \(p\), don&#39;t need so badly to do variable selection.)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-107" style="background:;">
  <hgroup>
    <h2>Aside: Danger, Danger!</h2>
  </hgroup>
  <article data-timings="">
    <h3>Multiple Hypothesis Testing</h3>

<p>In general, something inside you should shudder when facing the possibility of doing tens of thousands of tests on your data.</p>

<p>For variable selection, this is ok, but for very small perturbations
of this problem description things quickly go horribly, inexorably,
off the rails.</p>

<ul>
<li>&quot;Let&#39;s look through all pairwise combinations of my 100 features, looking for statistically significant correlations!&quot;</li>
<li>This comes up in cases where it&#39;s not necessarily obvious - looking for correlations between pixels in images.</li>
<li>If you are doing 10,000 hypothesis tests, using as your test for significance \(p < 0.05\), you <strong>expect</strong> 500 significant results --- even if the data is just random noise.</li>
<li>Eternal torment, gnashing of teeth, etc.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-108" style="background:;">
  <hgroup>
    <h2>Aside: Danger, Danger!</h2>
  </hgroup>
  <article data-timings="">
    <h3>Multiple Hypothesis Testing</h3>

<p>The canonical cautionary works on this subject:</p>

<ul>
<li><a href="http://xkcd.com/882/">http://xkcd.com/882/</a> (deals with the subject in XKCDs inimitable fashion)</li>
<li><a href="http://www.wired.com/2009/09/fmrisalmon/">http://www.wired.com/2009/09/fmrisalmon/</a> (finds statistically significant activation correlations in the brain of a salmon shown pictures of humans experiencing emotions.  The salmon &quot;was not alive at the time of scanning&quot;).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-109" style="background:;">
  <hgroup>
    <h2>Multiple Hypothesis Testing Corrections</h2>
  </hgroup>
  <article data-timings="">
    <p>There are a few methods of dealing with the situation of multiple
testing.  They all involve stricter definitions of what is significant
in this situation.  </p>

<p>Which is appropriate depends on the nature of your tests. Are they all independent? Identically distributed?</p>

<ul>
<li><strong>Bonferroni Correction</strong>: Safest choice; works regardless of independence, distribution.  Bluntest instrument.

<ul>
<li>If you otherwise would use a threshold \(\alpha\) for significance, and are doing \(k\) tests, use \(\alpha / k\).</li>
</ul></li>
<li><strong>idk correction</strong>: If independent:

<ul>
<li>use \((1 - (1-\alpha)^{(1/k)})\).</li>
</ul></li>
<li><strong>HolmBonferroni method</strong>: If independent, 

<ul>
<li>Sort results by significance, apply strictest Bonferroni correction only to most significant (i=1); in descending order, use \(\alpha/(k-i+1)\).</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-110" style="background:;">
  <hgroup>
    <h2>Multiple Hypothesis Testing Corrections</h2>
  </hgroup>
  <article data-timings="">
    <h3>End of Aside</h3>

<p>We won&#39;t cover this any further today, but it is vitally important to know
that both this is an issue, and there are ways of correcting for it.</p>

<p>Terms to look up if you find yourself in this situation:</p>

<ul>
<li>Family wise error rate</li>
<li>False discovery rate</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-111" style="background:;">
  <hgroup>
    <h2>Forward and Backward Selection</h2>
  </hgroup>
  <article data-timings="">
    <h3>How else to do variable selection</h3>

<p>Exhaustive search is safe, but infeasible in the most urgent cases.
(\(p = 100\) implies \(10^{30}\) model tests, and 100 isn&#39;t a huge
number of features.)</p>

<p>Two greedy methods are in common use, but can get caught in local minima:</p>

<ul>
<li>Forward selection: starting from nothing,

<ul>
<li>For each remaining feature, include it, and calculate adjusted CV error.</li>
<li>If for best feature, error drops enough, select it and continue</li>
<li>Else terminate and report selected features.</li>
</ul></li>
<li>Backward selection: same, but start with a model with all features, and drop until error rises too much.

<ul>
<li>sklearn: <code>RFECV</code> (Recursive Feature Elimination w/ CV)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-112" style="background:;">
  <hgroup>
    <h2>AIC, BIC, etc</h2>
  </hgroup>
  <article data-timings="">
    <p>How do you decide if a model with an additional feature is &quot;better&quot; than a simpler one?</p>

<ul>
<li>Always expect the more complex model to fit a little better.</li>
<li>Question is, is it better <em>enough</em> to justify the extra parameter?</li>
</ul>

<p>Two Information Criterion (AIC, BIC) consider the likelihood of the model, given the data, with a penalty for the number of
parameters in the model.</p>

<p>\[
\mathrm{xIC} \approx -2 \log L + k \alpha
\]</p>

<p>where \(L\) is the likelihood of the model, \(k\) is the number of
parameters, and \(\alpha = 2(1 + (k+1)/(n-k+1))\) for AIC and \(\log
n\) for BIC.</p>

<p>A decrease in AIC/BIC corresponds to a better model, taking into account the models&#39; complexity.</p>

<p>Correspond to different assumptions.  BIC is a little stricter,
which has advantages and disadvantages.  Either is defensible if used consistently.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-113" style="background:;">
  <hgroup>
    <h2>Variable Selection as Part of the Model</h2>
  </hgroup>
  <article data-timings="">
    <p>What we&#39;ve described so far is treating variable selection as a
general optimization problem, with the model construction as a
separate black box.</p>

<p>But in many cases, variable selection can be part of the model-building
step.</p>

<p>Decision trees: necessarily keep track of which features are most
important for dividing up the data.  In <code>sklearn</code>, in the fitted
decision tree model, the attribute <code>feature_importances_</code> is available
for ranking the discovered importance of the dimensions.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-114" style="background:;">
  <hgroup>
    <h2>Lasso/Ridge Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>There are <em>regularization</em> methods in regression which tamp down the coefficients of &quot;unimportant&quot; variables, or eliminate
them together, based on a fairly modest-looking change to how we specify the problem. </p>

<p>Recall that for generic least-squares regression, we are minimizing the MSE:
\[
\mathrm{MSE} = \left ( y - X_0 \beta_0 - X_1 \beta_1 - \dots - X_p \beta_p \right )^2 = \left ( y - X \beta \right )^2
\]
resulting in
\[
\hat{\beta} = \mathrm{argmin}_\beta \left ( y - X \beta \right )^2
\]</p>

<p>What we&#39;d like to do is keep the problem as simple as possible - one way of thinking of that in regression is forcing
the fit parameters to stay small; to constrain \(|\beta|\) in some way.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-115" style="background:;">
  <hgroup>
    <h2>Lasso/Ridge Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>Ridge and Lasso regression do a similar minimization, but with a penalty term for the coefficients:</p>

<p>Ridge:
\[
\hat{\beta} = \mathrm{argmin}_\beta \left ( y - X \beta \right )^2 + \lambda || \beta ||_2^2
\]</p>

<p>Lasso:
\[
\hat{\beta} = \mathrm{argmin}_\beta \left ( y - X \beta \right )^2 + \lambda || \beta ||_1
\]</p>

<p>In scikit-learn, these are available through <code>sklearn.linear_model.Lasso</code> and <code>.Ridge</code>.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-116" style="background:;">
  <hgroup>
    <h2>Lasso/Ridge Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>Both of these methods incur a penalty - the resulting regressions have a bias, as what we are optimizing for
is no longer solely to zero out mean squared error.</p>

<p>The reason to use Lasso/Ridge is when we are explicitly willing to trade some bias for more variance, and to 
simplify a model.  For the smoother penalty of ridge, this tradeoff can be written down explicitly for Ridge;
can only be numerically calculated for Lasso.</p>

<p>Because of the smoother penalty term in Ridge, cannot zero out variables \(\beta_i = 0\); thus it simplifies the model
in some sense, but doesn&#39;t properly do variable selection.  Lasso, however, will zero out coefficients.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-117" style="background:;">
  <hgroup>
    <h2>Lasso Regression</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>In either case, with Lasso or Ridge, coefficients decrease (usually smoothly) with increase in the regularization
parameter (in Lasso, \(\alpha\)).  </p>

<p>Ridge coefficients can pass through zero and change sign; not with Lasso.</p>

<p>An example can be seen in <code>scripts/regression/lasso.py</code></p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/featureselect/lasso-coeffs.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-118" style="background:;">
  <hgroup>
    <h2>Lasso Regression</h2>
  </hgroup>
  <article data-timings="">
    <p>Bias-Variance tradeoff; as we slowly bring down \(\alpha\), bias increases, but for quite a while it&#39;s more than balanced by
reduction in variance:</p>

<pre><code class="python">import scripts.regression.lasso as lasso
import numpy

X,y = lasso.getDiabetesData()
alphas = numpy.logspace(0.5,-3.5,20)
mses=[]
for alpha in alphas:
    mses.append(lasso.lassoFit(X,y,alpha=alpha))
print numpy.round(mses,2)
</code></pre>

<pre><code>## [ 3167.42  2979.89  3118.9   3000.94  3095.79  3427.5   3022.48  3152.64
##   3289.54  2907.83  2935.09  3105.    3138.35  3056.27  3230.43  3203.78
##   3316.99  3184.72  3030.19  3237.38]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-119" style="background:;">
  <hgroup>
    <h2>PCA</h2>
  </hgroup>
  <article data-timings="">
    <p>Properly speaking, PCA isn&#39;t variable selection - it&#39;s a technique
that allows dimension reduction in problems with purely continuous variables.</p>

<p>Doesn&#39;t look at the labels of the data; just imagines the data as points
in a \(p\)-dimensional space.</p>

<p>PCA is a transformation which rotates and scales the space into directions
defined by the variance in the data.  The direction in which the variance is 
highest is rotated to point along the first axes (the first principal component).
Next along the second axis, etc.</p>

<p>Increasingly higher dimensions are flatter and flatter, as they have less
variance.  The least significant principal components can often be profitably
ignored, as there is very little variation in those directions.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-120" style="background:;">
  <hgroup>
    <h2>PCA</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:48%;' class='centered'>
  <p>Note that PCA doesn&#39;t drop variables; rather, it generates combinations of all variables
in order of how significantly they vary.</p>

<p>One generally keeps most of the information from all variables, but expressed in a number
of combinations \(k\) &lt; \(p\).  Potentially best of both worlds - model simplicity without
throwing away information.</p>

</div>
<div style='float:right;width:48%;'>
  <p><img src="outputs/featureselect/pca-demo.png" alt=""></p>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-121" style="background:;">
  <hgroup>
    <h2>Variable Selection Hands-On</h2>
  </hgroup>
  <article data-timings="">
    <p>Take star98 demo from the beginning of this section, or the diabetes dataset from the lasso 
example, and play with lasso and ridge regression, or use a decision tree on the data and
examine important variables via the tree.  </p>

<p>Which variables are the most important?  Least?</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="dark segue nobackground" id="slide-122" style="background:;">
  <hgroup>
    <h2>Clustering</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-123" style="background:;">
  <hgroup>
    <h2>Clustering Overview</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-124" style="background:;">
  <hgroup>
    <h2>K-means</h2>
  </hgroup>
  <article data-timings="">
    <p>Foo bar baz</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-125" style="background:;">
  <hgroup>
    <h2>Hierarchical Clustering</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="dark segue nobackground" id="slide-126" style="background:;">
  <hgroup>
    <h2>Ensemble Methods</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-127" style="background:;">
  <hgroup>
    <h2>Bagging, Boosting</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-128" style="background:;">
  <hgroup>
    <h2>Random Forest</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="segue dark nobackground" id="slide-129" style="background:;">
  <hgroup>
    <h2>Conclusions</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-130" style="background:;">
  <hgroup>
    <h2>Concluding Points</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-131" style="background:;">
  <hgroup>
    <h2>Useful Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Cosma Shalizi, &quot;Advanced Data Analysis from an Elementary Point of View&quot;, <a href="http://www.stat.cmu.edu/%7Ecshalizi/ADAfaEPoV/">http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/</a>, covers regression, hypothesis testing, PCA, and density estimation, amongst other things, and is extremely lucidly written.</li>
<li>Andrew Ng, CS229 ML course, <a href="http://cs229.stanford.edu/materials.html">http://cs229.stanford.edu/materials.html</a>, and associated Coursera MOOC</li>
<li>Jure Leskovec, Anand Rajaraman, and Jeff Ullman, Mining of Massive Data Sets, pdf book online, <a href="http://www.mmds.org">http://www.mmds.org</a></li>
<li>Mohammed J. Zaki and Wagner Meira Jr., Data Mining and Analysis: Fundamental Concepts and Algorithms, <a href="http://www.cs.rpi.edu/%7Ezaki/PaperDir/DMABOOK.pdf">http://www.cs.rpi.edu/~zaki/PaperDir/DMABOOK.pdf</a></li>
<li>Scikit-Learn tutorial, <a href="http://scikit-learn.org/stable/tutorial/">http://scikit-learn.org/stable/tutorial/</a></li>
<li>Your SciNet Team</li>
<li>+ many others.<br></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title=''>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Techniques, Concepts'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='ML and Scientific Data Analysis'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='ML vs Scientific Data Analysis'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Scientific Data Analysis with ML'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Types of problems'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Types of Data'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Regression'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Regression'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Ordinary Least Squares (OLS)'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Note on Loss Functions'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Note on Loss Functions'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Polynomial Regression'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Polynomial Regression'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Polynomial Regression'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Polynomial Regression - In Sample Error'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Bias-Variance Decomposition'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Bias-Variance Decomposition'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='Bias, Variance in Polynomial Fitting'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='Polynomial Fitting - Constant'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='Polynomial Fitting - Linear'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='Polynomial Fitting - Seventh Order'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='Polynomial Fitting - Tenth Order'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='Polynomial Fitting - Twentyth Order'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='Bias and Consistency'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='Variance and Generalization'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='Bias-Variance Tradeoff'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='Choosing the Right Model'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='Training vs Validation'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='Training and Validation Hold Out Data'>
         30
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=31 title='Hands-On: Model Selection'>
         31
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=32 title='\(k\)-fold Cross Validation'>
         32
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=33 title='\(k\)-fold Cross Validation'>
         33
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=34 title='\(k\)-fold Cross Validation'>
         34
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=35 title='\(k\)-fold Cross Validation'>
         35
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=36 title='Resampling Aside #1'>
         36
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=37 title='Parametric Bootstrapping'>
         37
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=38 title='Non-parametric Bootstrapping'>
         38
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=39 title='Non-parametric Bootstrapping'>
         39
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=40 title='Hands On: Boostrapping Forest Fires'>
         40
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=41 title='Hands On: Boostrapping Forest Fires'>
         41
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=42 title='Hands On: Boostrapping Forest Fires'>
         42
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=43 title='Regression as Smoothing'>
         43
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=44 title='Regression with Nonparametric Smoothing'>
         44
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=45 title='Nonparametric Regression - Kernel'>
         45
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=46 title='Nonparametric Regression - LOWESS'>
         46
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=47 title='<code>statsmodels</code>'>
         47
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=48 title='Nonparametric Regression'>
         48
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=49 title='Parametric vs Nonparametric Regression'>
         49
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=50 title='Final Notes on Regression ... For Now'>
         50
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=51 title='Classification'>
         51
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=52 title='Classification'>
         52
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=53 title='Classification Problems'>
         53
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=54 title='Binary vs Multiclass Classification'>
         54
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=55 title='Outline'>
         55
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=56 title='Decision Trees'>
         56
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=57 title='Batman Decision Tree'>
         57
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=58 title='Splitting Algorithms'>
         58
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=59 title='Splitting Algorithms'>
         59
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=60 title='Splitting Algorithms'>
         60
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=61 title='Scikit-learn'>
         61
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=62 title='<code>sklearn</code>'>
         62
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=63 title='<code>pandas</code>'>
         63
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=64 title='<code>pandas</code>'>
         64
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=65 title='Trees and Overfitting'>
         65
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=66 title='Tree-pruning'>
         66
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=67 title='Hands on - Iris Data Set'>
         67
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=68 title='Nearest Neighbours - $k$NN'>
         68
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=69 title='Bias-Variance in $k$NN'>
         69
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=70 title='Bias-Variance in $k$NN'>
         70
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=71 title='Bias-Variance in $k$NN'>
         71
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=72 title='$k$NN and Geometry'>
         72
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=73 title='$k$NN and Geometry'>
         73
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=74 title='Scaling continuous features'>
         74
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=75 title='Digits data set'>
         75
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=76 title='Hands-on: kNN digits'>
         76
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=77 title='Hands-on: kNN digits'>
         77
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=78 title='Confusion matrix'>
         78
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=79 title='Evaluating Binary Classifiers'>
         79
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=80 title='Evaluating Binary Classifiers'>
         80
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=81 title='ROC Curve'>
         81
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=82 title='ROC Curve'>
         82
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=83 title='Evaluating Binary Classifiers'>
         83
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=84 title='Logistic Regression'>
         84
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=85 title='Logistic Regression'>
         85
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=86 title='Logistic Regression'>
         86
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=87 title='Logistic Regression'>
         87
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=88 title='Logistic Regression'>
         88
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=89 title='Logistic Regression Hands on'>
         89
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=90 title='Naive Bayes'>
         90
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=91 title='Naive Indeed'>
         91
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=92 title='Training Naive Bayes'>
         92
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=93 title='Training Naive Bayes'>
         93
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=94 title='Naive Like a Fox'>
         94
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=95 title='Usenet Newsgroups Classification'>
         95
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=96 title='Usenet Newsgroups Classification'>
         96
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=97 title='Text Processing Hands On'>
         97
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=98 title='Classification Summary'>
         98
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=99 title='Variable Selection'>
         99
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=100 title='Multivariate Linear Regression'>
         100
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=101 title='Multivariate Linear Regression'>
         101
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=102 title='Multivariate Linear Regression'>
         102
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=103 title='Multivariate Linear Regression'>
         103
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=104 title='How Not to do Variable Selection'>
         104
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=105 title='How to do a Little Variable Selection'>
         105
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=106 title='How (Maybe, but Hopefully Not) to do Variable Selection'>
         106
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=107 title='Aside: Danger, Danger!'>
         107
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=108 title='Aside: Danger, Danger!'>
         108
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=109 title='Multiple Hypothesis Testing Corrections'>
         109
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=110 title='Multiple Hypothesis Testing Corrections'>
         110
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=111 title='Forward and Backward Selection'>
         111
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=112 title='AIC, BIC, etc'>
         112
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=113 title='Variable Selection as Part of the Model'>
         113
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=114 title='Lasso/Ridge Regression'>
         114
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=115 title='Lasso/Ridge Regression'>
         115
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=116 title='Lasso/Ridge Regression'>
         116
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=117 title='Lasso Regression'>
         117
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=118 title='Lasso Regression'>
         118
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=119 title='PCA'>
         119
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=120 title='PCA'>
         120
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=121 title='Variable Selection Hands-On'>
         121
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=122 title='Clustering'>
         122
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=123 title='Clustering Overview'>
         123
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=124 title='K-means'>
         124
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=125 title='Hierarchical Clustering'>
         125
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=126 title='Ensemble Methods'>
         126
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=127 title='Bagging, Boosting'>
         127
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=128 title='Random Forest'>
         128
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=129 title='Conclusions'>
         129
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=130 title='Concluding Points'>
         130
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=131 title='Useful Resources'>
         131
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>